{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-09T11:01:42.217792Z","iopub.execute_input":"2022-04-09T11:01:42.218241Z","iopub.status.idle":"2022-04-09T11:01:42.241744Z","shell.execute_reply.started":"2022-04-09T11:01:42.218130Z","shell.execute_reply":"2022-04-09T11:01:42.241022Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:42.243310Z","iopub.execute_input":"2022-04-09T11:01:42.243657Z","iopub.status.idle":"2022-04-09T11:01:46.536439Z","shell.execute_reply.started":"2022-04-09T11:01:42.243605Z","shell.execute_reply":"2022-04-09T11:01:46.535676Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Since we're dealing with image files which are big files, GPU will be used for enhanced performance in terms of computations\n#Using Kaggle IDE, GPU is integrated & has been activated\n#Checking GPU Activation / Listing number of GPU\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(physical_devices))\ntf.config.experimental.set_memory_growth(physical_devices[0], True)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:46.537555Z","iopub.execute_input":"2022-04-09T11:01:46.537829Z","iopub.status.idle":"2022-04-09T11:01:46.692332Z","shell.execute_reply.started":"2022-04-09T11:01:46.537794Z","shell.execute_reply":"2022-04-09T11:01:46.691472Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nimport time\nimport shutil\nimport os\nos.listdir('/kaggle/input/')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:46.694103Z","iopub.execute_input":"2022-04-09T11:01:46.694583Z","iopub.status.idle":"2022-04-09T11:01:46.710356Z","shell.execute_reply.started":"2022-04-09T11:01:46.694537Z","shell.execute_reply":"2022-04-09T11:01:46.709588Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np #mathematical fn\nimport pandas as pd #data manipulation /analysis\nimport seaborn as sns #statistical data viz\nsns.set_style('darkgrid')\n\nimport glob #file/path retreival matching pattern\nimport matplotlib.pyplot as plt #ploting interface\n\nfrom tensorflow import keras #AI interface for TF Lib\n# from tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, GlobalAveragePooling2D, MaxPooling2D,BatchNormalization, Flatten #used in DL model layers\nfrom tensorflow.keras.optimizers import Adam, Adamax, RMSprop #to optimize performance","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:46.713684Z","iopub.execute_input":"2022-04-09T11:01:46.714534Z","iopub.status.idle":"2022-04-09T11:01:48.130146Z","shell.execute_reply.started":"2022-04-09T11:01:46.714492Z","shell.execute_reply":"2022-04-09T11:01:48.129356Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm #tracking bar\n\n#from matplotlib.pyplot import imshow #image in grayscale\n\nfrom IPython.core.display import display, HTML\n\nfrom PIL import Image #To represent Python Imaging Lib Image\nfrom sklearn.metrics import confusion_matrix, classification_report #evaluation stage","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:48.131514Z","iopub.execute_input":"2022-04-09T11:01:48.131788Z","iopub.status.idle":"2022-04-09T11:01:48.278167Z","shell.execute_reply.started":"2022-04-09T11:01:48.131755Z","shell.execute_reply":"2022-04-09T11:01:48.277486Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#NOW let's verify class distribution\n# NB: It is important to have a balanced / non-biased dataset  - to prevent model frombeing biased to majority class\n\ntrain_image_names = glob.glob('../input/foodnonfood/TRAIN/TRAIN/*/*.jpg') #READING OF TRAINIiNG imgs from path\nprint(\"Total number of training images: \", len(train_image_names))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:48.279653Z","iopub.execute_input":"2022-04-09T11:01:48.279906Z","iopub.status.idle":"2022-04-09T11:01:49.431128Z","shell.execute_reply.started":"2022-04-09T11:01:48.279870Z","shell.execute_reply":"2022-04-09T11:01:49.430280Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# make train_image_names as series object\ntrain_image_names = pd.Series(train_image_names)\ntrain_df = pd.DataFrame()\n\n# generate Filename field\ntrain_df['Filename'] = train_image_names.map(lambda img_name: img_name.split(\"/\")[-1])\n\n# generate ClassId field\ntrain_df['ClassId'] = train_image_names.map(lambda img_name: (img_name.split(\"/\")[-2]))\nclass_id_distribution = train_df['ClassId'].value_counts()\nclass_id_distribution.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:49.432375Z","iopub.execute_input":"2022-04-09T11:01:49.432658Z","iopub.status.idle":"2022-04-09T11:01:49.475297Z","shell.execute_reply.started":"2022-04-09T11:01:49.432605Z","shell.execute_reply":"2022-04-09T11:01:49.474326Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import skew\nclass_id_distribution.kurt(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:49.476646Z","iopub.execute_input":"2022-04-09T11:01:49.476948Z","iopub.status.idle":"2022-04-09T11:01:49.483283Z","shell.execute_reply.started":"2022-04-09T11:01:49.476913Z","shell.execute_reply":"2022-04-09T11:01:49.482445Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Now let's visualize the classes in the training; Here we see that the classes are balanced\n\nplt.figure(figsize=(10,5))\nplt.bar(class_id_distribution.index, class_id_distribution.values, color=['tomato','cornflowerblue'])\nplt.title(label=\"Training Set Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:49.484687Z","iopub.execute_input":"2022-04-09T11:01:49.485052Z","iopub.status.idle":"2022-04-09T11:01:49.678021Z","shell.execute_reply.started":"2022-04-09T11:01:49.485016Z","shell.execute_reply":"2022-04-09T11:01:49.677389Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#TEST dataset\ntest = glob.glob('../input/foodnonfood/TEST/TEST/*/*.jpg')\nprint(\"Total number of testation images: \", len(test))\n\n# make train_image_names as serie object\ntest = pd.Series(test)\ntestdf = pd.DataFrame()\n\n# generate Filename field\ntestdf['Filename'] = test.map(lambda img_name: img_name.split(\"/\")[-1])\n\n# generate ClassId field\ntestdf['ClassId'] = test.map(lambda img_name: (img_name.split(\"/\")[-2]))\nclass_id_distribution = testdf['ClassId'].value_counts()\nclass_id_distribution.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:49.679303Z","iopub.execute_input":"2022-04-09T11:01:49.679751Z","iopub.status.idle":"2022-04-09T11:01:50.561830Z","shell.execute_reply.started":"2022-04-09T11:01:49.679716Z","shell.execute_reply":"2022-04-09T11:01:50.561156Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.bar(class_id_distribution.index, class_id_distribution.values, color=['tomato','cornflowerblue'])\nplt.title(label=\"Validation Set Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:50.565468Z","iopub.execute_input":"2022-04-09T11:01:50.567320Z","iopub.status.idle":"2022-04-09T11:01:50.793070Z","shell.execute_reply.started":"2022-04-09T11:01:50.567282Z","shell.execute_reply":"2022-04-09T11:01:50.792453Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras #AI interface for TF Lib\n# from tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, GlobalAveragePooling2D, MaxPooling2D,BatchNormalization, Flatten #used in DL model layers\nfrom tensorflow.keras.optimizers import Adam, Adamax, RMSprop #to optimize performance\nfrom tensorflow.keras.metrics import categorical_crossentropy #Computes the crossentropy metric between the labels and predictions.\nfrom tensorflow.keras import regularizers # Regularizers for applying penalties on layer parameters during optimization.\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator #For generating image Batches (with augmentation)\nfrom tensorflow.keras.models import Model, load_model, Sequential #seq to group layer stacks linearly into a Model (tf.keras.Model)\nfrom tensorflow.keras.applications import imagenet_utils\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:50.796986Z","iopub.execute_input":"2022-04-09T11:01:50.798965Z","iopub.status.idle":"2022-04-09T11:01:50.810119Z","shell.execute_reply.started":"2022-04-09T11:01:50.798927Z","shell.execute_reply":"2022-04-09T11:01:50.809352Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#importing pre-trained cnn models\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.utils import plot_model\n\n\nfrom tqdm import tqdm #tracking bar\n\n#from matplotlib.pyplot import imshow #image in grayscale\n\nfrom IPython.core.display import display, HTML\n\nfrom PIL import Image #To represent Python Imaging Lib Image\nfrom sklearn.metrics import confusion_matrix, classification_report #evaluation stage","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:50.816051Z","iopub.execute_input":"2022-04-09T11:01:50.817958Z","iopub.status.idle":"2022-04-09T11:01:50.827053Z","shell.execute_reply.started":"2022-04-09T11:01:50.817922Z","shell.execute_reply":"2022-04-09T11:01:50.826417Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Augmentation using ImageData Generator\ntrain='../input/foodnonfood/TRAIN/TRAIN'\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255, #to transform every pixel value from range [0,255] -> [0,1]\n        #width_shift_range=0.2, #tried with no improvement\n        #height_shift_range=0.2,#tried with no improvement\n        shear_range=0.2,#shifting the pixels horizontally\n        zoom_range=0.2,\n        #rotation_range=20,#tried with no improvement\n        horizontal_flip=True,\n        fill_mode='nearest')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:50.830918Z","iopub.execute_input":"2022-04-09T11:01:50.833296Z","iopub.status.idle":"2022-04-09T11:01:50.840402Z","shell.execute_reply.started":"2022-04-09T11:01:50.833255Z","shell.execute_reply":"2022-04-09T11:01:50.839555Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#reading the training images\ntraining_set = train_datagen.flow_from_directory(\n                    train,\n                    target_size=(224, 224),\n                    batch_size=32,\n                    class_mode='binary')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:01:50.842922Z","iopub.execute_input":"2022-04-09T11:01:50.844203Z","iopub.status.idle":"2022-04-09T11:02:03.530321Z","shell.execute_reply.started":"2022-04-09T11:01:50.844164Z","shell.execute_reply":"2022-04-09T11:02:03.528674Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#reading test img\ntest='../input/foodnonfood/TEST/TEST/'\ntest_datagen=ImageDataGenerator(rescale=1./255)\ntestset=test_datagen.flow_from_directory(test,target_size=(224,224),batch_size=32,class_mode='binary')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:06:18.684136Z","iopub.execute_input":"2022-04-09T11:06:18.684456Z","iopub.status.idle":"2022-04-09T11:06:19.410371Z","shell.execute_reply.started":"2022-04-09T11:06:18.684413Z","shell.execute_reply":"2022-04-09T11:06:19.409580Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 15)) #image displayed in size 15X15\nfor i in range(0, 10):\n    plt.subplot(2, 5, i+1) #2 rows with 5 images per row\n    for X_batch, Y_batch in training_set:\n        image = X_batch[0]        \n        dic = {1:'food', 0:'non-food'} \n        plt.title(dic.get(Y_batch[0]))\n        plt.axis('off')\n        plt.imshow(np.squeeze(image),interpolation='nearest') # nearest will display an image without trying to interpolate between pixels since output display resolution != image resolution\n        break\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:42:31.120573Z","iopub.execute_input":"2022-04-09T11:42:31.121208Z","iopub.status.idle":"2022-04-09T11:42:36.041014Z","shell.execute_reply.started":"2022-04-09T11:42:31.121165Z","shell.execute_reply":"2022-04-09T11:42:36.040031Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Build model\n","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport tensorflow as tf\nimport glob\nfrom tensorflow.keras.optimizers import RMSprop\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nimport tempfile\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.optimizers import SGD, Adam, Adamax\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt #PLOTTING PERF  \nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator #used during image prep\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, MaxPooling2D #layers to be used in the CNN arch\nfrom keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score #used for evaluation Metrics\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:06:26.903527Z","iopub.execute_input":"2022-04-09T11:06:26.904090Z","iopub.status.idle":"2022-04-09T11:06:26.926007Z","shell.execute_reply.started":"2022-04-09T11:06:26.904053Z","shell.execute_reply":"2022-04-09T11:06:26.925344Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"testset.classes","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:06:31.650235Z","iopub.execute_input":"2022-04-09T11:06:31.650806Z","iopub.status.idle":"2022-04-09T11:06:31.656214Z","shell.execute_reply.started":"2022-04-09T11:06:31.650767Z","shell.execute_reply":"2022-04-09T11:06:31.655461Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#shape of the sets\nprint(\"training set shape is\",training_set[-1])\nprint(\"test set shape is\",testset[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:07:27.454427Z","iopub.execute_input":"2022-04-09T11:07:27.455089Z","iopub.status.idle":"2022-04-09T11:07:27.462444Z","shell.execute_reply.started":"2022-04-09T11:07:27.455052Z","shell.execute_reply":"2022-04-09T11:07:27.461758Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#verifying current image shapes\nprint(\"training images shape is\",training_set.image_shape)\nprint(\"tes images shape is\",testset.image_shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:07:55.968270Z","iopub.execute_input":"2022-04-09T11:07:55.968536Z","iopub.status.idle":"2022-04-09T11:07:55.974413Z","shell.execute_reply.started":"2022-04-09T11:07:55.968508Z","shell.execute_reply":"2022-04-09T11:07:55.973633Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"Adapted= Sequential()\nAdapted.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=[224,224,3]))\nAdapted.add(MaxPool2D(pool_size=2,strides=2))\nAdapted.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\nAdapted.add(MaxPool2D(pool_size=2,strides=2))\nAdapted.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\nAdapted.add(MaxPool2D(pool_size=2,strides=2))\nAdapted.add(Conv2D(filters=256,kernel_size=(3,3),activation='relu'))\nAdapted.add(MaxPool2D(pool_size=2,strides=2))\nAdapted.add(Dropout(0.2))\nAdapted.add(Flatten())\nAdapted.add(Dropout(0.2))\nAdapted.add(Dense(units=512,activation='relu'))\nAdapted.add(Dense(units=128,activation='relu'))\nAdapted.add(Dense(units=64,activation='relu'))\nAdapted.add(Dense(units=1,activation='sigmoid')) #only one output required\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:11:38.683681Z","iopub.execute_input":"2022-04-09T11:11:38.683946Z","iopub.status.idle":"2022-04-09T11:11:38.771810Z","shell.execute_reply.started":"2022-04-09T11:11:38.683915Z","shell.execute_reply":"2022-04-09T11:11:38.771130Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#additional Metrics\nMETS = [\n      \n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n]\n#compiling model\n#binary crossentropy used since this is binary classification model; output has only two classes\nAdapted.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy',METS])\nAdapted.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:11:42.110503Z","iopub.execute_input":"2022-04-09T11:11:42.110784Z","iopub.status.idle":"2022-04-09T11:11:42.139693Z","shell.execute_reply.started":"2022-04-09T11:11:42.110755Z","shell.execute_reply":"2022-04-09T11:11:42.139015Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"History = Adapted.fit(training_set,\n                      validation_data=(testset),\n                      epochs=5,\n                      callbacks=[EarlyStopping(monitor='val_accuracy',patience=20)]) \n#Using early stopping so that if the validation accuracy does not increase 20X conscutively\n#model is then stopped; i.e not all 50 epochs will be completed","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:11:44.876428Z","iopub.execute_input":"2022-04-09T11:11:44.877052Z","iopub.status.idle":"2022-04-09T11:27:11.527722Z","shell.execute_reply.started":"2022-04-09T11:11:44.877012Z","shell.execute_reply":"2022-04-09T11:27:11.527000Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#*others used to catch other output being returned\n#evaluation of trained model onto the test set\nloss, acc, prec, recall, auc, *others = Adapted.evaluate(testset) \nacc","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:27:37.729452Z","iopub.execute_input":"2022-04-09T11:27:37.730158Z","iopub.status.idle":"2022-04-09T11:27:58.360771Z","shell.execute_reply.started":"2022-04-09T11:27:37.730117Z","shell.execute_reply":"2022-04-09T11:27:58.359994Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"testset.reset()\nx=np.concatenate([testset.next()[0] for i in range(testset.__len__())])\ny=np.concatenate([testset.next()[1] for i in range(testset.__len__())])\nprint(x.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:28:45.763078Z","iopub.execute_input":"2022-04-09T11:28:45.763331Z","iopub.status.idle":"2022-04-09T11:29:04.214261Z","shell.execute_reply.started":"2022-04-09T11:28:45.763302Z","shell.execute_reply":"2022-04-09T11:29:04.213494Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:29:26.610436Z","iopub.execute_input":"2022-04-09T11:29:26.611059Z","iopub.status.idle":"2022-04-09T11:29:26.618068Z","shell.execute_reply.started":"2022-04-09T11:29:26.611017Z","shell.execute_reply":"2022-04-09T11:29:26.617292Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"preds = Adapted.predict(testset,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:29:52.622214Z","iopub.execute_input":"2022-04-09T11:29:52.622476Z","iopub.status.idle":"2022-04-09T11:30:03.065338Z","shell.execute_reply.started":"2022-04-09T11:29:52.622445Z","shell.execute_reply":"2022-04-09T11:30:03.064660Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = preds.copy()\npredictions[predictions <= 0.5] = 0\npredictions[predictions > 0.5] = 1","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:30:08.904891Z","iopub.execute_input":"2022-04-09T11:30:08.905420Z","iopub.status.idle":"2022-04-09T11:30:08.909892Z","shell.execute_reply.started":"2022-04-09T11:30:08.905385Z","shell.execute_reply":"2022-04-09T11:30:08.908973Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:30:22.062702Z","iopub.execute_input":"2022-04-09T11:30:22.063232Z","iopub.status.idle":"2022-04-09T11:30:22.068331Z","shell.execute_reply.started":"2022-04-09T11:30:22.063195Z","shell.execute_reply":"2022-04-09T11:30:22.067668Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_true=y,y_pred=predictions,target_names =['NONFOOD','FOOD']))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:32:51.261286Z","iopub.execute_input":"2022-04-09T11:32:51.262047Z","iopub.status.idle":"2022-04-09T11:32:51.279761Z","shell.execute_reply.started":"2022-04-09T11:32:51.262000Z","shell.execute_reply":"2022-04-09T11:32:51.278871Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\ncm = pd.DataFrame(data=confusion_matrix(y, predictions, labels=[0, 1]),index=[\"Actual NF\", \"Actual Food\"],\ncolumns=[\"Predicted NF\", \"Predicted Food\"])\nimport seaborn as sns\nsns.heatmap(cm,annot=True,fmt=\"d\")","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:34:19.423438Z","iopub.execute_input":"2022-04-09T11:34:19.423713Z","iopub.status.idle":"2022-04-09T11:34:19.663166Z","shell.execute_reply.started":"2022-04-09T11:34:19.423683Z","shell.execute_reply":"2022-04-09T11:34:19.662441Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:49:03.158298Z","iopub.execute_input":"2022-04-09T11:49:03.158990Z","iopub.status.idle":"2022-04-09T11:49:03.166865Z","shell.execute_reply.started":"2022-04-09T11:49:03.158949Z","shell.execute_reply":"2022-04-09T11:49:03.166000Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(20,20))\nfor i in range(0+46,2+46):\n  plt.subplot(1, 2, (i-46)+1)\n  if preds[i, 0] >= 0.5: \n      out = ('{:.2%} probability of being  Food case'.format(preds[i][0]))\n      \n      \n  else: \n      out = ('{:.2%} probability of being Non-Food case'.format(1-preds[i][0]))\n      \n      \n\n  plt.title(out+\"\\n Actual case : \"+ dic.get(y[i]))    \n  plt.imshow(np.squeeze(x[i]))\n  plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:53:07.774669Z","iopub.execute_input":"2022-04-09T11:53:07.775210Z","iopub.status.idle":"2022-04-09T11:53:08.723582Z","shell.execute_reply.started":"2022-04-09T11:53:07.775163Z","shell.execute_reply":"2022-04-09T11:53:08.722970Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"Adapted.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:56:06.201554Z","iopub.execute_input":"2022-04-09T11:56:06.202115Z","iopub.status.idle":"2022-04-09T11:56:06.214760Z","shell.execute_reply.started":"2022-04-09T11:56:06.202076Z","shell.execute_reply":"2022-04-09T11:56:06.213630Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_layer='conv2d_8'","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:14:59.639455Z","iopub.execute_input":"2022-04-09T12:14:59.639794Z","iopub.status.idle":"2022-04-09T12:14:59.644067Z","shell.execute_reply.started":"2022-04-09T12:14:59.639757Z","shell.execute_reply":"2022-04-09T12:14:59.643352Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"plt.imshow(x[46])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:11:26.554773Z","iopub.execute_input":"2022-04-09T12:11:26.555277Z","iopub.status.idle":"2022-04-09T12:11:26.855130Z","shell.execute_reply.started":"2022-04-09T12:11:26.555241Z","shell.execute_reply":"2022-04-09T12:11:26.854456Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:12:56.537581Z","iopub.execute_input":"2022-04-09T12:12:56.538195Z","iopub.status.idle":"2022-04-09T12:12:56.543514Z","shell.execute_reply.started":"2022-04-09T12:12:56.538157Z","shell.execute_reply":"2022-04-09T12:12:56.542860Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"x[46].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:13:07.012236Z","iopub.execute_input":"2022-04-09T12:13:07.012737Z","iopub.status.idle":"2022-04-09T12:13:07.025458Z","shell.execute_reply.started":"2022-04-09T12:13:07.012685Z","shell.execute_reply":"2022-04-09T12:13:07.024252Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"x[-1]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:14:19.487880Z","iopub.execute_input":"2022-04-09T12:14:19.488422Z","iopub.status.idle":"2022-04-09T12:14:19.496154Z","shell.execute_reply.started":"2022-04-09T12:14:19.488386Z","shell.execute_reply":"2022-04-09T12:14:19.495106Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nimport cv2\n\n\ndef GradCam(model, imarr, layer_name, eps=1e-8): #epsil value def\n    \n\n    gMod = Model(\n\t\t\tinputs=[model.inputs],\n\t\t\toutputs=[model.get_layer(layer_name).output,\n\t\t\t\tmodel.output])\n    \n    with tf.GradientTape() as tape:\n      inputs = tf.cast(imarr, tf.float32)\n      (convOutputs, predictions) = gMod(inputs)\n      loss = predictions[:, 0]\n    gradientss = tape.gradient(loss, convOutputs)\n    \n    csCNVOut = tf.cast(convOutputs > 0, \"float32\")\n    csgradie = tf.cast(gradientss > 0, \"float32\")\n    guidedgradientss = csCNVOut * csgradie * gradientss\n    convOutputs = convOutputs[0]\n    guidedgradientss = guidedgradientss[0]\n    wgts = tf.reduce_mean(guidedgradientss, axis=(0, 1))\n    gdcam = tf.reduce_sum(tf.multiply(wgts, convOutputs), axis=-1)\n    (w, h) = (imarr.shape[2], imarr.shape[1])\n    hm = cv2.resize(gdcam.numpy(), (w, h))\n    numer = hm - np.min(hm)\n    denom = (hm.max() - hm.min()) + eps\n    hm = numer / denom\n    return hm\n\n\ndef sigmoid(x, a, b, c):\n    return c / (1 + np.exp(-a * (x-b)))\n\ndef supimp(img_bgr, gdcam, thresh, emphasize=False):\n    hm = cv2.resize(gdcam, (img_bgr.shape[1], img_bgr.shape[0]))\n    if emphasize:\n        hm = sigmoid(hm, 50, thresh, 1)\n    hm = np.uint8(255 * hm)\n    hm = cv2.applyColorMap(hm, cv2.COLORMAP_JET)\n    \n    hif = .8\n    supimpd_img = hm * hif + img_bgr\n    supimpd_img = np.minimum(supimpd_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n    supimpd_img_rgb = cv2.cvtColor(supimpd_img, cv2.COLOR_BGR2RGB)\n    \n    return supimpd_img_rgb","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:55:40.432225Z","iopub.execute_input":"2022-04-09T11:55:40.432654Z","iopub.status.idle":"2022-04-09T11:55:40.729200Z","shell.execute_reply.started":"2022-04-09T11:55:40.432601Z","shell.execute_reply":"2022-04-09T11:55:40.728430Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"img=x[46]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:22:22.401540Z","iopub.execute_input":"2022-04-09T12:22:22.401833Z","iopub.status.idle":"2022-04-09T12:22:22.405518Z","shell.execute_reply.started":"2022-04-09T12:22:22.401795Z","shell.execute_reply":"2022-04-09T12:22:22.404741Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"img = cv2.resize(img,(224,224))     # resize image to match model's expected sizing\nimg = np.reshape(img,[1,224,224,3]) ","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:22:24.274154Z","iopub.execute_input":"2022-04-09T12:22:24.274743Z","iopub.status.idle":"2022-04-09T12:22:24.292622Z","shell.execute_reply.started":"2022-04-09T12:22:24.274701Z","shell.execute_reply":"2022-04-09T12:22:24.291929Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"img.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:22:31.025192Z","iopub.execute_input":"2022-04-09T12:22:31.025449Z","iopub.status.idle":"2022-04-09T12:22:31.030319Z","shell.execute_reply.started":"2022-04-09T12:22:31.025420Z","shell.execute_reply":"2022-04-09T12:22:31.029652Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"grad_cam=GradCam(Adapted,img,last_layer)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:23:49.135888Z","iopub.execute_input":"2022-04-09T12:23:49.136190Z","iopub.status.idle":"2022-04-09T12:23:49.166106Z","shell.execute_reply.started":"2022-04-09T12:23:49.136158Z","shell.execute_reply":"2022-04-09T12:23:49.165325Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"grad_cam_supimpd = supimp(x[46], grad_cam, 0.5, emphasize=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:32:45.094920Z","iopub.execute_input":"2022-04-09T12:32:45.095173Z","iopub.status.idle":"2022-04-09T12:32:45.103393Z","shell.execute_reply.started":"2022-04-09T12:32:45.095145Z","shell.execute_reply":"2022-04-09T12:32:45.102663Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"grad_cam_supimpd","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:32:47.649513Z","iopub.execute_input":"2022-04-09T12:32:47.650089Z","iopub.status.idle":"2022-04-09T12:32:47.658432Z","shell.execute_reply.started":"2022-04-09T12:32:47.650049Z","shell.execute_reply":"2022-04-09T12:32:47.657548Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"plt.imshow(grad_cam_supimpd)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:33:04.783389Z","iopub.execute_input":"2022-04-09T12:33:04.783878Z","iopub.status.idle":"2022-04-09T12:33:05.032155Z","shell.execute_reply.started":"2022-04-09T12:33:04.783841Z","shell.execute_reply":"2022-04-09T12:33:05.031472Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 15))\nax = plt.subplot(1, 2, 1)\nplt.imshow(x[46])\nplt.axis('off')\nplt.title('Original Image')\nax = plt.subplot(1, 2, 2)\nplt.imshow(grad_cam_supimpd)\nplt.axis('off')\nplt.title('Last CONV Grad-CAM heat-map')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:32:18.683156Z","iopub.execute_input":"2022-04-09T12:32:18.683744Z","iopub.status.idle":"2022-04-09T12:32:19.343064Z","shell.execute_reply.started":"2022-04-09T12:32:18.683702Z","shell.execute_reply":"2022-04-09T12:32:19.342425Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"plt.imshow(x[47])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:35:01.346364Z","iopub.execute_input":"2022-04-09T12:35:01.346656Z","iopub.status.idle":"2022-04-09T12:35:01.630749Z","shell.execute_reply.started":"2022-04-09T12:35:01.346603Z","shell.execute_reply":"2022-04-09T12:35:01.630097Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"img2=x[47]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:33:56.154233Z","iopub.execute_input":"2022-04-09T12:33:56.154557Z","iopub.status.idle":"2022-04-09T12:33:56.158134Z","shell.execute_reply.started":"2022-04-09T12:33:56.154523Z","shell.execute_reply":"2022-04-09T12:33:56.157318Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"img2 = cv2.resize(img2,(224,224))     # resize image to match model's expected sizing\nimg2 = np.reshape(img2,[1,224,224,3]) ","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:34:21.759443Z","iopub.execute_input":"2022-04-09T12:34:21.759716Z","iopub.status.idle":"2022-04-09T12:34:21.764572Z","shell.execute_reply.started":"2022-04-09T12:34:21.759687Z","shell.execute_reply":"2022-04-09T12:34:21.763671Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"grad_cam2=GradCam(Adapted,img2,last_layer)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:35:59.696668Z","iopub.execute_input":"2022-04-09T12:35:59.697195Z","iopub.status.idle":"2022-04-09T12:35:59.721309Z","shell.execute_reply.started":"2022-04-09T12:35:59.697157Z","shell.execute_reply":"2022-04-09T12:35:59.720651Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"grad_cam_supimpd2 = supimp(x[47], grad_cam2, 0.5, emphasize=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:36:49.583519Z","iopub.execute_input":"2022-04-09T12:36:49.584235Z","iopub.status.idle":"2022-04-09T12:36:49.590581Z","shell.execute_reply.started":"2022-04-09T12:36:49.584197Z","shell.execute_reply":"2022-04-09T12:36:49.589582Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 15))\nax = plt.subplot(1, 2, 1)\nplt.imshow(x[47])\nplt.axis('off')\nplt.title('Original Image')\nax = plt.subplot(1, 2, 2)\nplt.imshow(grad_cam_supimpd2)\nplt.axis('off')\nplt.title('Last CONV Grad-CAM heat-map')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T12:36:53.123328Z","iopub.execute_input":"2022-04-09T12:36:53.123588Z","iopub.status.idle":"2022-04-09T12:36:53.766947Z","shell.execute_reply.started":"2022-04-09T12:36:53.123557Z","shell.execute_reply":"2022-04-09T12:36:53.766269Z"},"trusted":true},"execution_count":154,"outputs":[]}]}